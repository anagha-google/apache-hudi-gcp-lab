{"cells":[{"cell_type":"markdown","id":"37b5211b","metadata":{},"source":["# Unit 1: Reading Hudi datasets with Spark-Scala\n","\n","In Module 2, we created a Hudi dataset. We also registered the dataset into a Hive Metastore/Dataproc Metastore Service as an external table. \n","\n","In this module:\n","1. We will review reading Hudi datasets from your data lake using Spark Dataframe API\n","2. Also review reading via Spark SQL, directly, the previously registered external table in the Apache Hive Metastore/Dataproc Metastore Service\n","\n","\n","At the end of this module, you should know how to read Hudi datasets from Spark."]},{"cell_type":"code","execution_count":1,"id":"f39fcb38","metadata":{"scrolled":true},"outputs":[],"source":["import sys.process._\n","import org.apache.hudi.QuickstartUtils._\n","import scala.collection.JavaConversions._\n","import org.apache.spark.sql.SaveMode._\n","import org.apache.hudi.DataSourceReadOptions._\n","import org.apache.hudi.DataSourceWriteOptions._\n","import org.apache.hudi.config.HoodieWriteConfig._\n","import org.apache.hudi.common.model.HoodieRecord"]},{"cell_type":"code","execution_count":2,"id":"f543a2ee","metadata":{"scrolled":true},"outputs":[{"ename":"Unknown Error","evalue":"<console>:2: error: ';' expected but '.' found.\n         .appName(\"Hudi-Learning-Module-01\") \\\n         ^\n","output_type":"error","traceback":[]}],"source":["val spark = SparkSession.builder \\\n","  .appName(\"Hudi-Learning-Unit-01-scala\") \\\n","  .master(\"yarn\")\\\n","  .enableHiveSupport()\\\n","  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n","  .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n","  .getOrCreate()"]},{"cell_type":"code","execution_count":3,"id":"2aa25f20","metadata":{"scrolled":true},"outputs":[{"data":{"text/plain":["Waiting for a Spark session to start..."]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["org.apache.spark.sql.SparkSession@7b2fa00a"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":null,"id":"f0eac727","metadata":{"scrolled":true},"outputs":[],"source":["val PROJECT_ID=\"gcloud config get-value core/project\" !!"]},{"cell_type":"code","execution_count":null,"id":"d9cdcdae","metadata":{},"outputs":[],"source":["val PROJECT_NBR=s\"gcloud projects describe $PROJECT_ID --format=value(projectNumber)\" !!"]},{"cell_type":"code","execution_count":6,"id":"81c23b16","metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Project ID is apache-hudi-lab\n","\n","Project ID is 623600433888\n","\n"]}],"source":["println(s\"Project ID is $PROJECT_ID\")\n","println(s\"Project ID is $PROJECT_NBR\")"]},{"cell_type":"code","execution_count":null,"id":"252e15d4","metadata":{"scrolled":true},"outputs":[],"source":["val PERSIST_TO_BUCKET = s\"gs://gaia_data_bucket-$PROJECT_NBR\"\n","val PARQUET_BASE_GCS_URI = s\"$PERSIST_TO_BUCKET/nyc-taxi-trips-parquet/\"\n","val HUDI_BASE_GCS_URI = s\"$PERSIST_TO_BUCKET/nyc-taxi-trips-hudi/\"\n","val DATABASE_NAME = \"taxi_db\"\n","val TABLE_NAME = \"nyc_taxi_trips_hudi\""]},{"cell_type":"markdown","id":"2bd7c057","metadata":{},"source":["## 1. Read Hudi dataset from source files in Cloud Storage, with Spark Dataframe API, and analyze with Spark SQL"]},{"cell_type":"code","execution_count":null,"id":"0470f5ea","metadata":{},"outputs":[],"source":["val tripsDF = spark.\n","  read.\n","  format(\"hudi\").\n","  load(HUDI_BASE_GCS_URI)"]},{"cell_type":"code","execution_count":9,"id":"2a070461","metadata":{},"outputs":[{"data":{"text/plain":["20939415"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["tripsDF.count()"]},{"cell_type":"code","execution_count":10,"id":"b85ea8d1","metadata":{},"outputs":[],"source":["tripsDF.createOrReplaceTempView(\"hudi_taxi_trips_snapshot\")"]},{"cell_type":"code","execution_count":11,"id":"8bacfa90","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+----------+\n","|trip_year|trip_count|\n","+---------+----------+\n","|     2019|   8023712|\n","|     2020|   4179576|\n","|     2022|   4022129|\n","|     2021|   4713998|\n","+---------+----------+\n","\n"]}],"source":["spark.sql(\"select trip_year,count(*) as trip_count from hudi_taxi_trips_snapshot group by trip_year\").show()"]},{"cell_type":"markdown","id":"3cbf70ef","metadata":{},"source":["## 2. Read previously registered external table on the same Hudi dataset in Hive Metsatore/Dataproc Metastore and analyze with Spark SQL"]},{"cell_type":"code","execution_count":12,"id":"05ec3710","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n","+---------+----------+\n","|trip_year|trip_count|\n","+---------+----------+\n","|     2019|   8023712|\n","|     2020|   4179576|\n","|     2022|   4022129|\n","|     2021|   4713998|\n","+---------+----------+\n","\n"]}],"source":["spark.sql(\"select trip_year,count(*) as trip_count from taxi_db.nyc_taxi_trips_hudi group by trip_year\").show()"]},{"cell_type":"markdown","id":"4a03d683","metadata":{},"source":["This concludes the unit 1. Proceed to the next notebook."]},{"cell_type":"code","execution_count":13,"id":"cf2f0a55","metadata":{},"outputs":[{"data":{"application/javascript":["Jupyter.notebook.session.delete();"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["%%javascript\n","Jupyter.notebook.session.delete();"]}],"metadata":{"kernelspec":{"display_name":"Apache Toree - Scala","language":"scala","name":"apache_toree_scala"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"2.12.15"}},"nbformat":4,"nbformat_minor":5}