{"cells": [{"cell_type": "markdown", "id": "78d90903", "metadata": {}, "source": "# Unit 6: Reading Hudi \"Merge On Read\" datasets with PySpark\nIn Module 2, we created a Hudi \"Merge On Reaad\" (MoR) dataset. We also registered the dataset into a Hive Metastore/Dataproc Metastore Service as an external table.\n\nIn this unit:\n\nWe will review reading Hudi MoR datasets from your data lake using Spark Dataframe API\nAlso review reading via Spark SQL, directly, the previously registered external table in the Apache Hive Metastore/Dataproc Metastore Service\nAt the end of this module, you should know how to read Hudi MoR datasets from Spark.\n\nThere are multiple read types possible with MoR table types that we will cover subsequently."}, {"cell_type": "code", "execution_count": 2, "id": "d1096055", "metadata": {}, "outputs": [], "source": "spark = SparkSession.builder \\\n  .appName(\"Hudi-Learning-Unit-06-pyspark\") \\\n  .master(\"yarn\")\\\n  .enableHiveSupport()\\\n  .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n  .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n  .getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "id": "9628d01e", "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://gaia-dpgce-cpu-623600433888-m.us-central1-a.c.apache-hudi-lab.internal:41241\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7fa2bf795030>"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 5, "id": "578ba5a2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Project ID is apache-hudi-lab\nProject Number is 623600433888\nBase path of Hudi dataset is gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-mor/\n"}], "source": "PROJECT_ID_OUTPUT=!gcloud config get-value core/project\nPROJECT_ID=PROJECT_ID_OUTPUT[0]\nPROJECT_NBR_OUTPUT=!gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\nPROJECT_NBR=PROJECT_NBR_OUTPUT[0]\nHUDI_BASE_GCS_URI = f\"gs://gaia_data_bucket-{PROJECT_NBR}/nyc-taxi-trips-hudi-mor/\"\n\nprint(f\"Project ID is {PROJECT_ID}\")\nprint(f\"Project Number is {PROJECT_NBR}\")\nprint(f\"Base path of Hudi dataset is {HUDI_BASE_GCS_URI}\")"}, {"cell_type": "markdown", "id": "c768867d", "metadata": {}, "source": "## 1. Read Hudi dataset from source files in Cloud Storage, with Spark Dataframe API, and analyze with Spark SQL against a temporary table"}, {"cell_type": "code", "execution_count": 6, "id": "0aba75ec", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/08/01 14:44:19 WARN GhfsStorageStatistics: Detected potential high latency for operation op_open. latencyMs=102; previousMaxLatencyMs=0; operationCount=1; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-mor/.hoodie/hoodie.properties\n23/08/01 14:44:20 WARN GhfsStorageStatistics: Detected potential high latency for operation stream_read_operations. latencyMs=220; previousMaxLatencyMs=0; operationCount=1; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-mor/.hoodie/hoodie.properties\n"}], "source": "tripsDF = spark.read.format(\"hudi\").load(HUDI_BASE_GCS_URI)"}, {"cell_type": "code", "execution_count": 7, "id": "da7530cc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "186263929"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "tripsDF.count()"}, {"cell_type": "code", "execution_count": 8, "id": "c1d743b8", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/08/01 14:45:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"}], "source": "tripsDF.createOrReplaceTempView(\"hudi_taxi_trips_snapshot\")"}, {"cell_type": "code", "execution_count": 9, "id": "7a42afc2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 6:====================================================>(2232 + 9) / 2241]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+----------+\n|trip_year|trip_count|\n+---------+----------+\n|     2021|  31972637|\n|     2019|  90897542|\n|     2022|  37023925|\n|     2020|  26369825|\n+---------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Without partition key\nspark.sql(\"select trip_year,count(*) as trip_count from hudi_taxi_trips_snapshot group by trip_year\").show()"}, {"cell_type": "markdown", "id": "e7eb350a", "metadata": {}, "source": "## 2. Read previously registered external table on the same Hudi dataset in Hive Metsatore/Dataproc Metastore and analyze with Spark SQL"}, {"cell_type": "code", "execution_count": 10, "id": "8004104d", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n[Stage 12:===================================================>(2233 + 8) / 2241]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+----------+\n| trip_date|trip_count|\n+----------+----------+\n|2022-12-07|         3|\n|2022-12-06|         3|\n|2022-12-01|        66|\n|2022-11-30|    120020|\n|2022-11-29|    121600|\n|2022-11-28|    108743|\n|2022-11-27|     92095|\n|2022-11-26|    101256|\n|2022-11-25|     88323|\n|2022-11-24|     71200|\n|2022-11-23|    107921|\n|2022-11-22|    116825|\n|2022-11-21|    110717|\n|2022-11-20|     82719|\n|2022-11-19|     96767|\n|2022-11-18|     97693|\n|2022-11-17|     97458|\n|2022-11-16|     94731|\n|2022-11-15|     92818|\n|2022-11-14|     84078|\n+----------+----------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# With partition key\nspark.sql(f\"select trip_date,count(*) as trip_count from taxi_db.nyc_taxi_trips_hudi_mor group by trip_date order by trip_date desc\").show()"}, {"cell_type": "markdown", "id": "3b815b3a", "metadata": {}, "source": "This concludes this unit. Proceed to the next notebook."}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}