{"cells": [{"cell_type": "markdown", "id": "78d90903", "metadata": {}, "source": "# Unit 7: Other miscellanrous features \nIn this unit, we will learn about other miscellaneous features and use the CoW table we created earlier.<br>\n\n"}, {"cell_type": "markdown", "id": "0166b45c", "metadata": {}, "source": "This unit takes about 5 minutes to complete."}, {"cell_type": "markdown", "id": "8781866b-f784-411c-8555-5db002b80d10", "metadata": {}, "source": "### Initialize Spark Session"}, {"cell_type": "code", "execution_count": 1, "id": "d1096055", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/08/01 18:48:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}, {"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://gaia-dpgce-cpu-623600433888-m.us-central1-a.c.apache-hudi-lab.internal:39883\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7fb9d5e3afe0>"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql.functions import lit\nfrom functools import reduce\nfrom pyspark.sql.types import LongType\nimport pyspark.sql.functions as F\nfrom datetime import datetime\n\nspark = SparkSession.builder \\\n  .appName(\"Hudi-Learning-Unit-07-PySpark\") \\\n  .master(\"yarn\") \\\n  .enableHiveSupport() \\\n  .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n  .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n  .getOrCreate()\n\nspark"}, {"cell_type": "markdown", "id": "adaa90d1", "metadata": {}, "source": "### Variables"}, {"cell_type": "code", "execution_count": 2, "id": "578ba5a2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Project ID is apache-hudi-lab\nProject number is 623600433888\nProject location is us-central1\nHudi base Cow table GCS URI is gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow\nDataproc Metastore Service thrift URI is thrift://10.60.192.28:9080\nTrip date to be used for deletes is 2021-05-25\n"}], "source": "PROJECT_ID_OUTPUT=!gcloud config get-value core/project\nPROJECT_ID=PROJECT_ID_OUTPUT[0]\nPROJECT_NBR_OUTPUT=!gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\nPROJECT_NBR=PROJECT_NBR_OUTPUT[0]\n\nTRIP_DATE='2021-05-25'\nLOCATION=\"us-central1\"\nHUDI_COW_BASE_GCS_URI = f\"gs://gaia_data_bucket-{PROJECT_NBR}/nyc-taxi-trips-hudi-cow\"\nDATAPROC_METASTORE_THRIFT_URI_LIST=!gcloud metastore services list --location $LOCATION | grep thrift | cut -d' ' -f11\nDATAPROC_METASTORE_THRIFT_URI=DATAPROC_METASTORE_THRIFT_URI_LIST[0]\n\nprint(f\"Project ID is {PROJECT_ID}\")\nprint(f\"Project number is {PROJECT_NBR}\")\nprint(f\"Project location is {LOCATION}\")\nprint(f\"Hudi base Cow table GCS URI is {HUDI_COW_BASE_GCS_URI}\")\nprint(f\"Dataproc Metastore Service thrift URI is {DATAPROC_METASTORE_THRIFT_URI}\")\nprint(f\"Trip date to be used for deletes is {TRIP_DATE}\")"}, {"cell_type": "markdown", "id": "a192314f-9014-4cac-8e4f-45bea6ea8d43", "metadata": {}, "source": "## 1. Review \"Insert Overwrite\" feature\n\n\"Insert Overwrite\" operation can be faster than upsert for batch ETL jobs, that are recomputing entire target partitions at once (as opposed to incrementally updating the target tables). This is because, we are able to bypass indexing, precombining and other repartitioning steps in the upsert write path completely.<br><br>\n\nRead the documentation/sample at this link:<br>\nhttps://hudi.apache.org/docs/quick-start-guide#insert-overwrite"}, {"cell_type": "markdown", "id": "fe24ba25-fd52-491f-a20a-593fb6912806", "metadata": {}, "source": "## 2. Review \"Alter Table\" commands\n\nExercises for you:<br>\n1. Rename the cow table to something else. \n2. Add a column\n3. Set table properties to keep only past 5 commits\n\nCommands are at:<br>\nhttps://hudi.apache.org/docs/quick-start-guide#alter-table"}, {"cell_type": "markdown", "id": "331df52b-5c40-4d1d-afda-170571a9ec51", "metadata": {}, "source": "## 3. Review partition commands\n\nExercises for you:<br>\n1. Run a \"show partitions\" command\n2. Drop the partition trip_date='2021-12-31'\n\nCommands are at:<br>\nhttps://hudi.apache.org/docs/quick-start-guide#partition-sql-command"}, {"cell_type": "markdown", "id": "eb433c0c-6605-40c5-b03d-3856de918c61", "metadata": {}, "source": "This concludes the unit, please proceed to the next notebook."}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}, "toc-showcode": true}, "nbformat": 4, "nbformat_minor": 5}