{"cells": [{"cell_type": "markdown", "id": "78d90903", "metadata": {}, "source": "# Unit 6: Queries against CoW tables\n\nIn this unit, we will learn about query types against CoW tables.<br>\n\nWe will review various types of queries/operations:<br>\nCommit listing: This is a metadata listing of commits against a table<br>\nIncremental queries: Queries only see new data written to the table, since a given commit/compaction. This effectively provides change streams to enable incremental data pipelines.<br>\nTime travel queries: Queries can show the state of a record over time specified<br>\nPoint in time queries: Queries for data at a specific point in time<br>\n"}, {"cell_type": "markdown", "id": "0166b45c", "metadata": {}, "source": "This unit takes about 5 minutes to complete."}, {"cell_type": "markdown", "id": "8781866b-f784-411c-8555-5db002b80d10", "metadata": {}, "source": "### Initialize Spark Session"}, {"cell_type": "code", "execution_count": 1, "id": "d1096055", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/08/01 18:59:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}, {"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://gaia-dpgce-cpu-623600433888-m.us-central1-a.c.apache-hudi-lab.internal:43075\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f19e3279030>"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql.functions import lit\nfrom functools import reduce\nfrom pyspark.sql.types import LongType\nimport pyspark.sql.functions as F\nfrom datetime import datetime\n\nspark = SparkSession.builder \\\n  .appName(\"Hudi-Learning-Unit-06-PySpark\") \\\n  .master(\"yarn\") \\\n  .enableHiveSupport() \\\n  .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n  .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n  .getOrCreate()\n\nspark"}, {"cell_type": "markdown", "id": "adaa90d1", "metadata": {}, "source": "### Variables"}, {"cell_type": "code", "execution_count": 2, "id": "578ba5a2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Project ID is apache-hudi-lab\nProject number is 623600433888\nProject location is us-central1\nHudi base Cow table GCS URI is gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow\nDataproc Metastore Service thrift URI is thrift://10.60.192.28:9080\nTrip date to be used for deletes is 2021-01-31\n"}], "source": "PROJECT_ID_OUTPUT=!gcloud config get-value core/project\nPROJECT_ID=PROJECT_ID_OUTPUT[0]\nPROJECT_NBR_OUTPUT=!gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\nPROJECT_NBR=PROJECT_NBR_OUTPUT[0]\n\nTRIP_DATE='2021-01-31'\nLOCATION=\"us-central1\"\nHUDI_COW_BASE_GCS_URI = f\"gs://gaia_data_bucket-{PROJECT_NBR}/nyc-taxi-trips-hudi-cow\"\nDATAPROC_METASTORE_THRIFT_URI_LIST=!gcloud metastore services list --location $LOCATION | grep thrift | cut -d' ' -f11\nDATAPROC_METASTORE_THRIFT_URI=DATAPROC_METASTORE_THRIFT_URI_LIST[0]\n\nprint(f\"Project ID is {PROJECT_ID}\")\nprint(f\"Project number is {PROJECT_NBR}\")\nprint(f\"Project location is {LOCATION}\")\nprint(f\"Hudi base Cow table GCS URI is {HUDI_COW_BASE_GCS_URI}\")\nprint(f\"Dataproc Metastore Service thrift URI is {DATAPROC_METASTORE_THRIFT_URI}\")\nprint(f\"Trip date to be used for deletes is {TRIP_DATE}\")"}, {"cell_type": "markdown", "id": "a192314f-9014-4cac-8e4f-45bea6ea8d43", "metadata": {}, "source": "## 1. Listing the commits to a table"}, {"cell_type": "code", "execution_count": 3, "id": "7ddd7177-3bdf-445c-82b3-5c42f92b19bd", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n23/08/01 18:59:28 WARN GhfsStorageStatistics: Detected potential high latency for operation op_open. latencyMs=104; previousMaxLatencyMs=0; operationCount=1; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/.hoodie/hoodie.properties\n[Stage 1:=============================>                             (2 + 2) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----------------+-------------------+------------------------+---------------------+----------------------------+------------+\n|commit_time      |total_bytes_written|total_files_added|total_files_updated|total_partitions_written|total_records_written|total_update_records_written|total_errors|\n+-----------------+-------------------+-----------------+-------------------+------------------------+---------------------+----------------------------+------------+\n|20230801033306138|1355990            |0                |1                  |1                       |32603                |0                           |0           |\n|20230801033217250|1356037            |0                |1                  |1                       |32604                |1                           |0           |\n|20230801032820936|8891097            |0                |2                  |2                       |244892               |1                           |0           |\n|20230801032340309|8286843            |0                |1                  |1                       |257928               |1                           |0           |\n|20230801032242395|8286843            |0                |1                  |1                       |257928               |0                           |0           |\n|20230731210745246|1365789824         |337              |0                  |337                     |37023925             |0                           |0           |\n|20230731210155584|1204455850         |365              |0                  |365                     |31972637             |0                           |0           |\n|20230731205659112|960652331          |366              |0                  |366                     |26369825             |0                           |0           |\n|20230731203923454|3418884833         |939              |0                  |365                     |90897542             |0                           |0           |\n+-----------------+-------------------+-----------------+-------------------+------------------------+---------------------+----------------------------+------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql(\"call show_commits(table => 'taxi_db.nyc_taxi_trips_hudi_cow', limit => 100);\").show(100, truncate=False)"}, {"cell_type": "markdown", "id": "d0c7db8c-7105-4e65-89ba-bbc6ac3f6af1", "metadata": {}, "source": "## 2. Incremental queries\nQueries only see new data written to the table, since a given commit/compaction. This effectively provides change streams to enable incremental data pipelines"}, {"cell_type": "code", "execution_count": 4, "id": "f1d9f623-e813-4ea0-a37d-36618081f388", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/08/01 18:59:51 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"}], "source": "# Read the base data\nspark. \\\n  read. \\\n  format(\"hudi\"). \\\n  load(HUDI_COW_BASE_GCS_URI). \\\n  createOrReplaceTempView(\"hudi_cow_table\")"}, {"cell_type": "code", "execution_count": 5, "id": "2e5c2348-e6c6-4451-afcf-3e3d3a7d4b2a", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Begin commit time is: 20230801032340309\n20230731203923454\n20230731205659112\n20230731210155584\n20230731210745246\n20230801032340309\n20230801032820936\n"}], "source": "# Query for commits\ncommits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_cow_table order by commitTime\").collect()))\nbeginTime = commits[len(commits) - 2] # commit time we are interested in\n\nprint(f\"Begin commit time is: {beginTime}\")\n\n# The various commit timestamps are:\nfor commit in commits:\n    print(commit)"}, {"cell_type": "code", "execution_count": 6, "id": "90b12bad-880b-4fc9-900f-382046bb3495", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 14:==============================================>           (4 + 1) / 5]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+-------------------+--------------------+------------------+----------------------+--------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------------+---------+\n|_hoodie_commit_time|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|taxi_type|trip_year|trip_month|trip_day|trip_hour|trip_minute|vendor_id|    pickup_datetime|   dropoff_datetime|store_and_forward|rate_code|pickup_location_id|dropoff_location_id|passenger_count|trip_distance| fare_amount|  surcharge|    mta_tax| tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type_code|congestion_surcharge|trip_type|ehail_fee|partition_date|distance_between_service|time_between_service|      trip_id|trip_date|\n+-------------------+-------------------+--------------------+------------------+----------------------+--------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------------+---------+\n|  20230801032820936|  20230801032820936|20230801032820936...|     1786706865009|  trip_date=2019-03-10|340b03dc-b6e3-411...|   yellow|     2019|         3|      10|       14|         44|        2|2019-03-10 14:44:09|2019-03-10 14:59:03|                N|      1.0|               141|                112|              6|  3.810000000|15.000000000|0.300000000|0.500000000|3.660000000|        0E-9|                 null|21.960000000|                1|                null|     null|     null|    2019-03-10|                    null|                null|1786706865009|     null|\n|  20230801032820936|  20230801032820936|20230801032820936...|      695784702201|  trip_date=2019-01-15|ab386b38-4ae1-4e5...|   yellow|     2019|         1|      15|       15|          0|        1|2019-01-15 11:00:30|2019-01-15 11:49:46|                N|      1.0|                68|                141|              0|  3.400000000|28.000000000|0.300000000|0.500000000|7.200000000|        0E-9|                 null|36.000000000|                1|                null|     null|     null|    2019-01-15|                    null|                null| 695784702201|     null|\n+-------------------+-------------------+--------------------+------------------+----------------------+--------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+-----------+-----------+-----------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------------+---------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Incrementally query data\nincremental_read_options = {\n  'hoodie.datasource.query.type': 'incremental',\n  'hoodie.datasource.read.begin.instanttime': beginTime,\n}\n\ntripsIncrementalDF = spark.read.format(\"hudi\"). \\\n  options(**incremental_read_options). \\\n  load(HUDI_COW_BASE_GCS_URI)\ntripsIncrementalDF.createOrReplaceTempView(\"hudi_trips_incremental\")\n\nspark.sql(\"SELECT `_hoodie_commit_time`, * FROM  hudi_trips_incremental LIMIT 100\").show()\n# If you strictly followed the lab untis, notebook by notebook, the results should reflect the upsert notebook records "}, {"cell_type": "markdown", "id": "ab33d9cc-80df-4c29-84cd-8c71314ec689", "metadata": {}, "source": "## 3. Time Travel queries\nThe sample below shows the state of a trip from inception to latest commit - across commits."}, {"cell_type": "markdown", "id": "fbc9bbd8-0ce9-477e-b86a-b747567e9c07", "metadata": {}, "source": "### 3.1. Using Spark SQL against HMS\nReplace the query below with the trip ID of the UPDATE_CANDIDATE_TRIP_ID from notebook 4"}, {"cell_type": "code", "execution_count": 7, "id": "7deeb06d-083b-4743-8af4-ea1b4a24f898", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "As of commit timestamp: 20230731203923454\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|_hoodie_commit_time|     trip_id|taxi_type|vendor_id|    pickup_datetime|   dropoff_datetime|total_amount|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|  20230731203923454|695784702201|   yellow|        1|2019-01-15 10:00:30|2019-01-15 10:49:46|36.000000000|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n\nAs of commit timestamp: 20230731205659112\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|_hoodie_commit_time|     trip_id|taxi_type|vendor_id|    pickup_datetime|   dropoff_datetime|total_amount|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|  20230731203923454|695784702201|   yellow|        1|2019-01-15 10:00:30|2019-01-15 10:49:46|36.000000000|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n\nAs of commit timestamp: 20230731210155584\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|_hoodie_commit_time|     trip_id|taxi_type|vendor_id|    pickup_datetime|   dropoff_datetime|total_amount|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|  20230731203923454|695784702201|   yellow|        1|2019-01-15 10:00:30|2019-01-15 10:49:46|36.000000000|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n\nAs of commit timestamp: 20230731210745246\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|_hoodie_commit_time|     trip_id|taxi_type|vendor_id|    pickup_datetime|   dropoff_datetime|total_amount|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|  20230731203923454|695784702201|   yellow|        1|2019-01-15 10:00:30|2019-01-15 10:49:46|36.000000000|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n\nAs of commit timestamp: 20230801032340309\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|_hoodie_commit_time|     trip_id|taxi_type|vendor_id|    pickup_datetime|   dropoff_datetime|total_amount|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|  20230731203923454|695784702201|   yellow|        1|2019-01-15 10:00:30|2019-01-15 10:49:46|36.000000000|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n\nAs of commit timestamp: 20230801032820936\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|_hoodie_commit_time|     trip_id|taxi_type|vendor_id|    pickup_datetime|   dropoff_datetime|total_amount|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|  20230801032820936|695784702201|   yellow|        1|2019-01-15 11:00:30|2019-01-15 11:49:46|36.000000000|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n\n"}], "source": "for commit in commits:\n    print(f\"As of commit timestamp: {commit}\")\n    spark.sql(\"select _hoodie_commit_time,trip_id,taxi_type,vendor_id,pickup_datetime,\" \\\n              \"dropoff_datetime,total_amount \"\\\n              \"from taxi_db.nyc_taxi_trips_hudi_cow \"\\\n              f\"timestamp as of {commit} \"\\\n              \"Where trip_date='2019-01-15' AND trip_id=695784702201\").show()"}, {"cell_type": "markdown", "id": "537d27aa-1414-4c7b-8b74-6a1a08130621", "metadata": {}, "source": "### 3.2. Using Dataframe API"}, {"cell_type": "code", "execution_count": 8, "id": "e15ba180-5012-4d84-af4c-b6baf5fffb0e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "As of commit timestamp: 20230731203923454\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|_hoodie_commit_time|     trip_id|taxi_type|vendor_id|    pickup_datetime|   dropoff_datetime|total_amount|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|  20230731203923454|695784702201|   yellow|        1|2019-01-15 10:00:30|2019-01-15 10:49:46|36.000000000|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n\nAs of commit timestamp: 20230731205659112\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|_hoodie_commit_time|     trip_id|taxi_type|vendor_id|    pickup_datetime|   dropoff_datetime|total_amount|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|  20230731203923454|695784702201|   yellow|        1|2019-01-15 10:00:30|2019-01-15 10:49:46|36.000000000|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n\nAs of commit timestamp: 20230731210155584\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|_hoodie_commit_time|     trip_id|taxi_type|vendor_id|    pickup_datetime|   dropoff_datetime|total_amount|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|  20230731203923454|695784702201|   yellow|        1|2019-01-15 10:00:30|2019-01-15 10:49:46|36.000000000|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n\nAs of commit timestamp: 20230731210745246\n"}, {"name": "stderr", "output_type": "stream", "text": "23/08/01 19:01:25 WARN GhfsStorageStatistics: Detected potential high latency for operation stream_read_operations. latencyMs=509; previousMaxLatencyMs=66; operationCount=3778; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/.hoodie/hoodie.properties\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|_hoodie_commit_time|     trip_id|taxi_type|vendor_id|    pickup_datetime|   dropoff_datetime|total_amount|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|  20230731203923454|695784702201|   yellow|        1|2019-01-15 10:00:30|2019-01-15 10:49:46|36.000000000|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n\nAs of commit timestamp: 20230801032340309\n"}, {"name": "stderr", "output_type": "stream", "text": "23/08/01 19:01:32 WARN GhfsStorageStatistics: Detected potential high latency for operation stream_write_operations. latencyMs=413; previousMaxLatencyMs=11; operationCount=9410; context=gs://dataproc-temp-us-central1-623600433888-ojsvfynx/92fae947-ff2e-4e99-9649-afae6a74a071/spark-job-history/application_1690608046061_0120.inprogress\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|_hoodie_commit_time|     trip_id|taxi_type|vendor_id|    pickup_datetime|   dropoff_datetime|total_amount|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|  20230731203923454|695784702201|   yellow|        1|2019-01-15 10:00:30|2019-01-15 10:49:46|36.000000000|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n\nAs of commit timestamp: 20230801032820936\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|_hoodie_commit_time|     trip_id|taxi_type|vendor_id|    pickup_datetime|   dropoff_datetime|total_amount|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n|  20230801032820936|695784702201|   yellow|        1|2019-01-15 11:00:30|2019-01-15 11:49:46|36.000000000|\n+-------------------+------------+---------+---------+-------------------+-------------------+------------+\n\n"}], "source": "from pyspark.sql.functions import col\n\nfor commit in commits:\n    print(f\"As of commit timestamp: {commit}\")\n    spark.read. \\\n      format(\"hudi\"). \\\n      option(\"as.of.instant\", commit) \\\n        .load(f\"{HUDI_COW_BASE_GCS_URI}/trip_date=2019-01-15\") \\\n        .filter((col(\"trip_id\")  == 695784702201)) \\\n        .select(\"_hoodie_commit_time\",\"trip_id\",\"taxi_type\",\"vendor_id\",\"pickup_datetime\",\"dropoff_datetime\",\"total_amount\") \\\n        .show()\n    \n"}, {"cell_type": "markdown", "id": "106f785f-9106-4079-b23f-4bc8b2e06938", "metadata": {}, "source": "## 4. Point in time queries\nCan be used to see the state of a record in a commit time range"}, {"cell_type": "markdown", "id": "f3264ea6-43a8-4a44-bf8c-585cc015349a", "metadata": {}, "source": "### 4.1. Using Dataframe API"}, {"cell_type": "code", "execution_count": null, "id": "b672daa7-5e6e-483e-adfc-6654ecb91990", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/08/01 19:01:42 WARN GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=283; previousMaxLatencyMs=273; operationCount=212; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2021-07-28/15fd4fed-fbc0-483a-8998-f906c438ca43-0_81-57-15502_20230731210155584.parquet\n23/08/01 19:01:42 WARN GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=330; previousMaxLatencyMs=283; operationCount=220; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-05-05/9650d10d-af6e-4106-adc3-8bca114be29d-0_407-19-9369_20230731203923454.parquet\n23/08/01 19:01:42 WARN GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=352; previousMaxLatencyMs=330; operationCount=223; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-04-23/6c7b99c7-71ad-4e91-8776-618277296276-0_155-19-9117_20230731203923454.parquet\n23/08/01 19:01:42 WARN GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=347; previousMaxLatencyMs=330; operationCount=222; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2021-03-25/b348300b-1dab-4cfa-a924-1b9b84f8dce9-0_13-57-15523_20230731210155584.parquet\n23/08/01 19:01:42 WARN GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=392; previousMaxLatencyMs=347; operationCount=233; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2020-03-21/96ae6b39-520a-47f9-94a0-187fbe8bb1cb-0_290-38-12755_20230731205659112.parquet\n23/08/01 19:01:42 WARN GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=439; previousMaxLatencyMs=392; operationCount=426; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2020-08-26/ad52c048-bac0-4aa6-ae6f-4f04f7250331-0_274-38-12740_20230731205659112.parquet\n[Stage 80:>                                                       (0 + 20) / 20]\r"}], "source": "# pyspark\nbeginTime = \"000\" # Represents all commits > this time.\nendTime = commits[len(commits) - 2]\n\n# query point in time data\npoint_in_time_read_options = {\n  'hoodie.datasource.query.type': 'incremental',\n  'hoodie.datasource.read.end.instanttime': endTime,\n  'hoodie.datasource.read.begin.instanttime': beginTime\n}\n\ntripsPointInTimeDF = spark.read.format(\"hudi\"). \\\n  options(**point_in_time_read_options). \\\n  load(f\"{HUDI_COW_BASE_GCS_URI}/trip_date=2019-01-15\"). \\\n  filter((col(\"trip_id\")  == 695784702201)). \\\n  select(\"_hoodie_commit_time\",\"trip_id\",\"taxi_type\",\"vendor_id\",\"pickup_datetime\",\"dropoff_datetime\",\"total_amount\"). \\\n  show()"}, {"cell_type": "markdown", "id": "8dd5641c-9cb2-4a26-b873-431d7cc0f1a2", "metadata": {}, "source": "### 4.2. Using SparkSQL against Dataproc Metastore Service"}, {"cell_type": "code", "execution_count": null, "id": "343c4515-bf83-4307-8cc8-8eac55b839dc", "metadata": {}, "outputs": [], "source": "spark.sql(\"SELECT _hoodie_commit_time,trip_id,taxi_type,vendor_id,pickup_datetime,\" \\\n              \"dropoff_datetime,total_amount \"\\\n              \"FROM taxi_db.nyc_taxi_trips_hudi_cow \"\\\n              \"WHERE trip_date='2019-01-15' AND trip_id=695784702201 \" \\\n              \"AND _hoodie_commit_time between 20230731203923454 and 20230801032820936\").show()"}, {"cell_type": "markdown", "id": "eb433c0c-6605-40c5-b03d-3856de918c61", "metadata": {}, "source": "This concludes the unit, please proceed to the next notebook."}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}, "toc-showcode": true}, "nbformat": 4, "nbformat_minor": 5}