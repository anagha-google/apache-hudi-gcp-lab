{"cells": [{"cell_type": "markdown", "id": "78d90903", "metadata": {}, "source": "# Unit 2: Reading Hudi \"Copy On Write\" datasets with PySpark\nIn Module 2, we created a Hudi dataset. We also registered the dataset into a Hive Metastore/Dataproc Metastore Service as an external table.\n\nIn this unit:\n\nWe will review reading Hudi datasets from your data lake using Spark Dataframe API\nAlso review reading via Spark SQL, directly, the previously registered external table in the Apache Hive Metastore/Dataproc Metastore Service\nAt the end of this module, you should know how to read Hudi datasets from Spark."}, {"cell_type": "code", "execution_count": 1, "id": "d1096055", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/08/01 03:18:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder \\\n  .appName(\"Hudi-Learning-Unit-02-pyspark\") \\\n  .master(\"yarn\")\\\n  .enableHiveSupport()\\\n  .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n  .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n  .getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "id": "9628d01e", "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://gaia-dpgce-cpu-623600433888-m.us-central1-a.c.apache-hudi-lab.internal:43793\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f70e31b9030>"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 3, "id": "578ba5a2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Project ID is apache-hudi-lab\nProject Number is 623600433888\nBase path of Hudi dataset is gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/\n"}], "source": "PROJECT_ID_OUTPUT=!gcloud config get-value core/project\nPROJECT_ID=PROJECT_ID_OUTPUT[0]\nPROJECT_NBR_OUTPUT=!gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\nPROJECT_NBR=PROJECT_NBR_OUTPUT[0]\nHUDI_BASE_GCS_URI = f\"gs://gaia_data_bucket-{PROJECT_NBR}/nyc-taxi-trips-hudi-cow/\"\n\nprint(f\"Project ID is {PROJECT_ID}\")\nprint(f\"Project Number is {PROJECT_NBR}\")\nprint(f\"Base path of Hudi dataset is {HUDI_BASE_GCS_URI}\")"}, {"cell_type": "markdown", "id": "c768867d", "metadata": {}, "source": "## 1. Read Hudi dataset from source files in Cloud Storage, with Spark Dataframe API, and analyze with Spark SQL against a temporary table"}, {"cell_type": "code", "execution_count": 4, "id": "0aba75ec", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/08/01 03:18:46 WARN GhfsStorageStatistics: Detected potential high latency for operation op_open. latencyMs=106; previousMaxLatencyMs=0; operationCount=1; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/.hoodie/hoodie.properties\n                                                                                \r"}], "source": "tripsDF = spark.read.format(\"hudi\").load(HUDI_BASE_GCS_URI)"}, {"cell_type": "code", "execution_count": 5, "id": "da7530cc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "186263929"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "tripsDF.count()"}, {"cell_type": "code", "execution_count": 6, "id": "c1d743b8", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/08/01 03:19:27 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"}], "source": "tripsDF.createOrReplaceTempView(\"hudi_taxi_trips_snapshot\")"}, {"cell_type": "code", "execution_count": 7, "id": "7a42afc2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 6:=====================================================> (113 + 3) / 116]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+----------+\n|trip_year|trip_count|\n+---------+----------+\n|     2020|  26369825|\n|     2022|  37023925|\n|     2019|  90897542|\n|     2021|  31972637|\n+---------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Without partition key\nspark.sql(\"select trip_year,count(*) as trip_count from hudi_taxi_trips_snapshot group by trip_year\").show()"}, {"cell_type": "markdown", "id": "e7eb350a", "metadata": {}, "source": "## 2. Read previously registered external table on the same Hudi dataset in Hive Metsatore/Dataproc Metastore and analyze with Spark SQL"}, {"cell_type": "code", "execution_count": 8, "id": "8004104d", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n[Stage 12:====================================================> (112 + 4) / 116]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+----------+\n| trip_date|trip_count|\n+----------+----------+\n|2022-12-07|         3|\n|2022-12-06|         3|\n|2022-12-01|        66|\n|2022-11-30|    120020|\n|2022-11-29|    121600|\n|2022-11-28|    108743|\n|2022-11-27|     92095|\n|2022-11-26|    101256|\n|2022-11-25|     88323|\n|2022-11-24|     71200|\n|2022-11-23|    107921|\n|2022-11-22|    116825|\n|2022-11-21|    110717|\n|2022-11-20|     82719|\n|2022-11-19|     96767|\n|2022-11-18|     97693|\n|2022-11-17|     97458|\n|2022-11-16|     94731|\n|2022-11-15|     92818|\n|2022-11-14|     84078|\n+----------+----------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "23/08/01 03:20:26 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /hadoop/spark/tmp/blockmgr-379abfa9-68bf-478c-a48d-2b8937dfdda2. Falling back to Java IO way\njava.io.IOException: Failed to delete: /hadoop/spark/tmp/blockmgr-379abfa9-68bf-478c-a48d-2b8937dfdda2\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171) ~[spark-network-common_2.12-3.3.0.jar:3.3.0]\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110) ~[spark-network-common_2.12-3.3.0.jar:3.3.0]\n\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91) ~[spark-network-common_2.12-3.3.0.jar:3.3.0]\n\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1206) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:374) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:370) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36) ~[scala-library-2.12.14.jar:?]\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33) ~[scala-library-2.12.14.jar:?]\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198) ~[scala-library-2.12.14.jar:?]\n\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:370) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n\tat org.apache.spark.storage.DiskBlockManager.$anonfun$addShutdownHook$2(DiskBlockManager.scala:352) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.14.jar:?]\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.14.jar:?]\n\tat scala.util.Try$.apply(Try.scala:213) ~[scala-library-2.12.14.jar:?]\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178) ~[spark-core_2.12-3.3.0.jar:3.3.0]\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]\n\tat java.lang.Thread.run(Thread.java:829) ~[?:?]\nERROR:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n    raise Py4JNetworkError(\"Answer from Java side is empty\")\npy4j.protocol.Py4JNetworkError: Answer from Java side is empty\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending or receiving\n/usr/lib/spark/python/pyspark/context.py:561: RuntimeWarning: Unable to cleanly shutdown Spark JVM process. It is possible that the process has crashed, been killed or may also be in a zombie state.\n  warnings.warn(\n"}], "source": "# With partition key\nspark.sql(f\"select trip_date,count(*) as trip_count from taxi_db.nyc_taxi_trips_hudi_cow group by trip_date order by trip_date desc\").show()"}, {"cell_type": "markdown", "id": "3b815b3a", "metadata": {}, "source": "This concludes the unit 1. Proceed to the next notebook."}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}