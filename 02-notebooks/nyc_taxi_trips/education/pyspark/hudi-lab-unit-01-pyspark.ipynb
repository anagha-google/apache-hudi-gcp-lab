{"cells": [{"cell_type": "markdown", "id": "78d90903", "metadata": {}, "source": "# Unit 1: Reading Hudi datasets with PySpark\nIn Module 2, we created a Hudi dataset. We also registered the dataset into a Hive Metastore/Dataproc Metastore Service as an external table.\n\nIn this unit:\n\nWe will review reading Hudi datasets from your data lake using Spark Dataframe API\nAlso review reading via Spark SQL, directly, the previously registered external table in the Apache Hive Metastore/Dataproc Metastore Service\nAt the end of this module, you should know how to read Hudi datasets from Spark."}, {"cell_type": "code", "execution_count": 1, "id": "d1096055", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/07/30 02:09:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "spark = SparkSession.builder \\\n  .appName(\"Hudi-Learning-Unit-01-pyspark\") \\\n  .master(\"yarn\")\\\n  .enableHiveSupport()\\\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n  .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n  .getOrCreate()"}, {"cell_type": "code", "execution_count": 2, "id": "9628d01e", "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://gaia-dpgce-cpu-623600433888-m.us-central1-a.c.apache-hudi-lab.internal:37095\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f4738ba2fe0>"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 3, "id": "578ba5a2", "metadata": {}, "outputs": [], "source": "PROJECT_ID_OUTPUT=!gcloud config get-value core/project\nPROJECT_ID=PROJECT_ID_OUTPUT[0]"}, {"cell_type": "code", "execution_count": 4, "id": "f014aae5", "metadata": {}, "outputs": [], "source": "PROJECT_NBR_OUTPUT=!gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\nPROJECT_NBR=PROJECT_NBR_OUTPUT[0]"}, {"cell_type": "code", "execution_count": 5, "id": "0f4a8679", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Project ID is apache-hudi-lab\nProject Number is 623600433888\n"}], "source": "print(f\"Project ID is {PROJECT_ID}\")\nprint(f\"Project Number is {PROJECT_NBR}\")"}, {"cell_type": "code", "execution_count": 6, "id": "91e51a3b", "metadata": {}, "outputs": [], "source": "PERSIST_TO_BUCKET = f\"gs://gaia_data_bucket-{PROJECT_NBR}\"\nPARQUET_BASE_GCS_URI = f\"{PERSIST_TO_BUCKET}/nyc-taxi-trips-parquet/\"\nHUDI_BASE_GCS_URI = f\"{PERSIST_TO_BUCKET}/nyc-taxi-trips-hudi-cow/\"\nDATABASE_NAME = \"taxi_db\"\nTABLE_NAME = \"nyc_taxi_trips_hudi_cow\""}, {"cell_type": "markdown", "id": "c768867d", "metadata": {}, "source": "## 1. Read Hudi dataset from source files in Cloud Storage, with Spark Dataframe API, and analyze with Spark SQL against a temporary table"}, {"cell_type": "code", "execution_count": 7, "id": "0aba75ec", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/07/30 02:09:57 WARN GhfsStorageStatistics: Detected potential high latency for operation op_open. latencyMs=106; previousMaxLatencyMs=0; operationCount=1; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/.hoodie/hoodie.properties\n"}], "source": "tripsDF = spark.read.format(\"hudi\").load(HUDI_BASE_GCS_URI)"}, {"cell_type": "code", "execution_count": 8, "id": "da7530cc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "186263929"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "tripsDF.count()"}, {"cell_type": "code", "execution_count": 9, "id": "c1d743b8", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/07/30 02:10:48 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"}], "source": "tripsDF.createOrReplaceTempView(\"hudi_taxi_trips_snapshot\")"}, {"cell_type": "code", "execution_count": 10, "id": "7a42afc2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 6:====================================================>(2240 + 1) / 2241]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+----------+\n|trip_year|trip_count|\n+---------+----------+\n|     2020|  26369825|\n|     2019|  90897542|\n|     2022|  37023925|\n|     2021|  31972637|\n+---------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Without partition key\nspark.sql(\"select trip_year,count(*) as trip_count from hudi_taxi_trips_snapshot group by trip_year\").show()"}, {"cell_type": "markdown", "id": "e7eb350a", "metadata": {}, "source": "## 2. Read previously registered external table on the same Hudi dataset in Hive Metsatore/Dataproc Metastore and analyze with Spark SQL"}, {"cell_type": "code", "execution_count": 11, "id": "8004104d", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n[Stage 12:===================================================>(2238 + 3) / 2241]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+----------+\n| trip_date|trip_count|\n+----------+----------+\n|2022-12-07|         3|\n|2022-12-06|         3|\n|2022-12-01|        66|\n|2022-11-30|    120020|\n|2022-11-29|    121600|\n|2022-11-28|    108743|\n|2022-11-27|     92095|\n|2022-11-26|    101256|\n|2022-11-25|     88323|\n|2022-11-24|     71200|\n|2022-11-23|    107921|\n|2022-11-22|    116825|\n|2022-11-21|    110717|\n|2022-11-20|     82719|\n|2022-11-19|     96767|\n|2022-11-18|     97693|\n|2022-11-17|     97458|\n|2022-11-16|     94731|\n|2022-11-15|     92818|\n|2022-11-14|     84078|\n+----------+----------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# With partition key\nspark.sql(f\"select trip_date,count(*) as trip_count from {DATABASE_NAME}.{TABLE_NAME} group by trip_date order by trip_date desc\").show()"}, {"cell_type": "markdown", "id": "3b815b3a", "metadata": {}, "source": "This concludes the unit 1. Proceed to the next notebook."}, {"cell_type": "code", "execution_count": 12, "id": "a5eaa198", "metadata": {}, "outputs": [{"data": {"application/javascript": "Jupyter.notebook.session.delete();\n", "text/plain": "<IPython.core.display.Javascript object>"}, "metadata": {}, "output_type": "display_data"}], "source": "%%javascript\nJupyter.notebook.session.delete();"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}