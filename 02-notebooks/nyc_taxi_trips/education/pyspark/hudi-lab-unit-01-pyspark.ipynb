{"cells": [{"cell_type": "markdown", "id": "78d90903", "metadata": {}, "source": "# Unit 1: Reading Hudi datasets with PySpark\nIn Module 2, we created a Hudi dataset. We also registered the dataset into a Hive Metastore/Dataproc Metastore Service as an external table.\n\nIn this module:\n\nWe will review reading Hudi datasets from your data lake using Spark Dataframe API\nAlso review reading via Spark SQL, directly, the previously registered external table in the Apache Hive Metastore/Dataproc Metastore Service\nAt the end of this module, you should know how to read Hudi datasets from Spark."}, {"cell_type": "code", "execution_count": 18, "id": "d1096055", "metadata": {}, "outputs": [], "source": "spark = SparkSession.builder \\\n  .appName(\"Hudi-Learning-Unit-01-pyspark\") \\\n  .master(\"yarn\")\\\n  .enableHiveSupport()\\\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n  .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n  .getOrCreate()"}, {"cell_type": "code", "execution_count": 19, "id": "9628d01e", "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://gaia-dpgce-cpu-623600433888-m.us-central1-a.c.apache-hudi-lab.internal:43619\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f57e90deb60>"}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "code", "execution_count": 20, "id": "578ba5a2", "metadata": {}, "outputs": [], "source": "PROJECT_ID_OUTPUT=!gcloud config get-value core/project\nPROJECT_ID=PROJECT_ID_OUTPUT[0]"}, {"cell_type": "code", "execution_count": 21, "id": "f014aae5", "metadata": {}, "outputs": [], "source": "PROJECT_NBR_OUTPUT=!gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\nPROJECT_NBR=PROJECT_NBR_OUTPUT[0]"}, {"cell_type": "code", "execution_count": 22, "id": "0f4a8679", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Project ID is apache-hudi-lab\nProject Number is 623600433888\n"}], "source": "print(f\"Project ID is {PROJECT_ID}\")\nprint(f\"Project Number is {PROJECT_NBR}\")"}, {"cell_type": "code", "execution_count": 23, "id": "91e51a3b", "metadata": {}, "outputs": [], "source": "PERSIST_TO_BUCKET = f\"gs://gaia_data_bucket-{PROJECT_NBR}\"\nPARQUET_BASE_GCS_URI = f\"{PERSIST_TO_BUCKET}/nyc-taxi-trips-parquet/\"\nHUDI_BASE_GCS_URI = f\"{PERSIST_TO_BUCKET}/nyc-taxi-trips-hudi-cow/\"\nDATABASE_NAME = \"taxi_db\"\nTABLE_NAME = \"nyc_taxi_trips_hudi\""}, {"cell_type": "markdown", "id": "c768867d", "metadata": {}, "source": "## 1. Read Hudi dataset from source files in Cloud Storage, with Spark Dataframe API, and analyze with Spark SQL"}, {"cell_type": "code", "execution_count": 24, "id": "0aba75ec", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "tripsDF = spark.read.format(\"hudi\").load(HUDI_BASE_GCS_URI)"}, {"cell_type": "code", "execution_count": 25, "id": "da7530cc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "185550246"}, "execution_count": 25, "metadata": {}, "output_type": "execute_result"}], "source": "tripsDF.count()"}, {"cell_type": "code", "execution_count": 26, "id": "c1d743b8", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/07/24 16:43:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"}], "source": "tripsDF.createOrReplaceTempView(\"hudi_taxi_trips_snapshot\")"}, {"cell_type": "code", "execution_count": 27, "id": "7a42afc2", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 10:=====================================================>(129 + 2) / 131]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+----------+\n|trip_year|trip_count|\n+---------+----------+\n|     2020|  26192443|\n|     2022|  36821513|\n|     2021|  31845761|\n|     2019|  90690529|\n+---------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql(\"select trip_year,count(*) as trip_count from hudi_taxi_trips_snapshot group by trip_year\").show()"}, {"cell_type": "markdown", "id": "e7eb350a", "metadata": {}, "source": "## 2. Read previously registered external table on the same Hudi dataset in Hive Metsatore/Dataproc Metastore and analyze with Spark SQL"}, {"cell_type": "code", "execution_count": 28, "id": "8004104d", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n[Stage 20:=====================================================>(129 + 2) / 131]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+----------+\n|trip_year|trip_count|\n+---------+----------+\n|     2020|  26192443|\n|     2022|  36821513|\n|     2021|  31845761|\n|     2019|  90690529|\n+---------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql(\"select trip_year,count(*) as trip_count from taxi_db.nyc_taxi_trips_hudi group by trip_year\").show()"}, {"cell_type": "markdown", "id": "3b815b3a", "metadata": {}, "source": "This concludes the unit 1. Proceed to the next notebook."}, {"cell_type": "code", "execution_count": 29, "id": "a5eaa198", "metadata": {}, "outputs": [{"data": {"application/javascript": "Jupyter.notebook.session.delete();\n", "text/plain": "<IPython.core.display.Javascript object>"}, "metadata": {}, "output_type": "display_data"}], "source": "%%javascript\nJupyter.notebook.session.delete();"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}