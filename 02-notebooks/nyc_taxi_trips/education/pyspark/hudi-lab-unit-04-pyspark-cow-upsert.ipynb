{"cells": [{"cell_type": "markdown", "id": "78d90903", "metadata": {}, "source": "# Unit 4: UPSERT into COW tables\nIn this unit, we will learn upsert operations into COW tables.<br>\nThis unit takes about 5 minutes to complete.\n"}, {"cell_type": "markdown", "id": "8781866b-f784-411c-8555-5db002b80d10", "metadata": {}, "source": "### Initialize Spark Session"}, {"cell_type": "code", "execution_count": 1, "id": "d1096055", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/07/30 02:28:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}, {"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://gaia-dpgce-cpu-623600433888-m.us-central1-a.c.apache-hudi-lab.internal:39675\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>PySparkShell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7fe122936fe0>"}, "execution_count": 1, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql.functions import lit\nfrom functools import reduce\nfrom pyspark.sql.types import LongType\nimport pyspark.sql.functions as F\nfrom datetime import datetime\n\nspark = SparkSession.builder \\\n  .appName(\"Hudi-Learning-Unit-06-PySpark\") \\\n  .master(\"yarn\")\\\n  .enableHiveSupport()\\\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n  .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n  .getOrCreate()\n\nspark"}, {"cell_type": "markdown", "id": "adaa90d1", "metadata": {}, "source": "### Declare & define variables"}, {"cell_type": "code", "execution_count": 2, "id": "578ba5a2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Project ID is apache-hudi-lab\nProject Number is 623600433888\n"}], "source": "PROJECT_ID_OUTPUT=!gcloud config get-value core/project\nPROJECT_ID=PROJECT_ID_OUTPUT[0]\nPROJECT_NBR_OUTPUT=!gcloud projects describe $PROJECT_ID --format=\"value(projectNumber)\"\nPROJECT_NBR=PROJECT_NBR_OUTPUT[0]\nprint(f\"Project ID is {PROJECT_ID}\")\nprint(f\"Project Number is {PROJECT_NBR}\")\n\nPERSIST_TO_BUCKET = f\"gs://gaia_data_bucket-{PROJECT_NBR}\"\nHUDI_COW_BASE_GCS_URI = f\"{PERSIST_TO_BUCKET}/nyc-taxi-trips-hudi-cow\"\nDATABASE_NAME = \"taxi_db\"\nCOW_TABLE_NAME = \"nyc_taxi_trips_hudi_cow\""}, {"cell_type": "markdown", "id": "eb8cb7af-22a9-4465-b8ed-de2e4627c3e6", "metadata": {}, "source": "## 1. Upsert into Hudi"}, {"cell_type": "markdown", "id": "4e63b74b-e299-4f48-bb26-e6db78a8ad26", "metadata": {}, "source": "Here we will learn how to an insert a new record & and update existing record(s) via the upsert operation.<br>\nJust like we did in the previous exercise, we will take some existing record and increment the hour and use in this lab unit.<br>\n\nInsert candidate trip_date to clone & morph: '2019-03-15' <br>\nUpdate candidate trip_date to clone & morph: '2019-01-18'<br>"}, {"cell_type": "markdown", "id": "8e14ed76-68dc-405d-98b8-843150a0d5c9", "metadata": {}, "source": "### 1.1. Trips to clone and use for the lab unit\n"}, {"cell_type": "code", "execution_count": 3, "id": "192a3ebe-098f-42fc-a978-b480719f5b65", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "INSERT_CLONE_CANDIDATE_TRIP_ID: 1760936629794\nINSERT_CANDIDATE_PREDICATES: trip_date=\"2019-03-10\" AND trip_id=1760936629794\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 4:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "UPDATE_CLONE_CANDIDATE_TRIP_ID: 83\nUPDATE_CANDIDATE_PREDICATES: trip_date=\"2019-01-15\" AND trip_id=83\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "INSERT_TRIP_DATE='2019-03-10'\nINSERT_TRIP_DATE_PREDICATE=f\"trip_date=\\\"{INSERT_TRIP_DATE}\\\"\"\nINSERT_CLONE_CANDIDATE_TRIP_ID=spark.sql(f\"select trip_id  from {DATABASE_NAME}.{COW_TABLE_NAME} WHERE {INSERT_TRIP_DATE_PREDICATE} LIMIT 1\").collect()[0][0]\nprint(f\"INSERT_CLONE_CANDIDATE_TRIP_ID: {INSERT_CLONE_CANDIDATE_TRIP_ID}\")\nINSERT_CANDIDATE_PREDICATES=f\"{INSERT_TRIP_DATE_PREDICATE} AND trip_id={INSERT_CLONE_CANDIDATE_TRIP_ID}\"\nprint(f\"INSERT_CANDIDATE_PREDICATES: {INSERT_CANDIDATE_PREDICATES}\")\n\nUPDATE_TRIP_DATE='2019-01-15'\nUPDATE_TRIP_DATE_PREDICATE=f\"trip_date=\\\"{UPDATE_TRIP_DATE}\\\"\"\nUPDATE_CANDIDATE_TRIP_ID=spark.sql(f\"select trip_id  from {DATABASE_NAME}.{COW_TABLE_NAME} WHERE {UPDATE_TRIP_DATE_PREDICATE} LIMIT 1\").collect()[0][0]\nprint(f\"UPDATE_CLONE_CANDIDATE_TRIP_ID: {UPDATE_CANDIDATE_TRIP_ID}\")\nUPDATE_CANDIDATE_PREDICATES=f\"{UPDATE_TRIP_DATE_PREDICATE} AND trip_id={UPDATE_CANDIDATE_TRIP_ID}\"\nprint(f\"UPDATE_CANDIDATE_PREDICATES: {UPDATE_CANDIDATE_PREDICATES}\")"}, {"cell_type": "markdown", "id": "8b608893-d788-4637-8f0a-f70afc87417d", "metadata": {}, "source": "### 1.2. Generate unique Trip ID for the insert "}, {"cell_type": "code", "execution_count": 4, "id": "720b96ed-ef17-44bf-8c90-a209910d1bd6", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 5:=============================>                             (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Insert trip ID is: 1786706865009\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "TO_BE_INSERTED_TRIP_ID=spark.sql(f\"SELECT max(trip_id) AS max_trip_id FROM {DATABASE_NAME}.{COW_TABLE_NAME} WHERE {INSERT_TRIP_DATE_PREDICATE}\").collect()[0][0] + 1\nprint(f\"Insert trip ID is: {TO_BE_INSERTED_TRIP_ID}\")"}, {"cell_type": "markdown", "id": "4c29ecbf-63d2-437f-a6a9-d09ec36930f4", "metadata": {}, "source": "### 1.3. Insert dataframe creation"}, {"cell_type": "code", "execution_count": 5, "id": "177dd65a-15d6-4740-b383-e3acb10a0e5f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/07/30 02:29:19 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n[Stage 9:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+--------------------+------------------+----------------------+--------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+---------+-----------+----------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------------+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|taxi_type|trip_year|trip_month|trip_day|trip_hour|trip_minute|vendor_id|    pickup_datetime|   dropoff_datetime|store_and_forward|rate_code|pickup_location_id|dropoff_location_id|passenger_count|trip_distance| fare_amount|surcharge|    mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type_code|congestion_surcharge|trip_type|ehail_fee|partition_date|distance_between_service|time_between_service|      trip_id| trip_date|\n+-------------------+--------------------+------------------+----------------------+--------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+---------+-----------+----------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------------+----------+\n|  20230729053557449|20230729053557449...|     1760936629794|  trip_date=2019-03-10|7d98900a-981b-45a...|    green|     2019|         3|      10|        9|         26|        2|2019-03-10 09:26:21|2019-03-10 09:50:20|                N|      5.0|               177|                 89|              1|  4.450000000|15.360000000|     0E-9|0.500000000|      0E-9|        0E-9|                 null|15.860000000|              1.0|                null|      2.0|     null|    2019-03-10|                    null|                null|1760936629794|2019-03-10|\n+-------------------+--------------------+------------------+----------------------+--------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+---------+-----------+----------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Original record\ninsertCandidateTripDFCow=spark.sql(f\"SELECT * FROM {DATABASE_NAME}.{COW_TABLE_NAME} WHERE {INSERT_CANDIDATE_PREDICATES}\")\ninsertCandidateTripDFCow.show()\n"}, {"cell_type": "code", "execution_count": 6, "id": "f9d9d168-e6db-441d-a786-8cd80f40326d", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 11:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+---------+-----------+----------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------------+----------+\n|_hoodie_partition_path|taxi_type|trip_year|trip_month|trip_day|trip_hour|trip_minute|vendor_id|pickup_datetime    |dropoff_datetime   |store_and_forward|rate_code|pickup_location_id|dropoff_location_id|passenger_count|trip_distance|fare_amount |surcharge|mta_tax    |tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type_code|congestion_surcharge|trip_type|ehail_fee|partition_date|distance_between_service|time_between_service|trip_id      |trip_date |\n+----------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+---------+-----------+----------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------------+----------+\n|trip_date=2019-03-10  |green    |2019     |3         |10      |14       |26         |2        |2019-03-10 14:26:21|2019-03-10 14:50:20|N                |5.0      |177               |89                 |1              |4.450000000  |15.360000000|0E-9     |0.500000000|0E-9      |0E-9        |null                 |15.860000000|1.0              |null                |2.0      |null     |2019-03-10    |null                    |null                |1786706865009|2019-03-10|\n+----------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+---------+-----------+----------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Generate record to insert based off of the above record - increment all the date column by 5 hours\ninsertTripDFCow = insertCandidateTripDFCow.withColumn('pickup_datetime', insertCandidateTripDFCow.pickup_datetime + F.expr('INTERVAL 5 HOURS')) \\\n                                    .withColumn('dropoff_datetime', insertCandidateTripDFCow.dropoff_datetime + F.expr('INTERVAL 5 HOURS')) \\\n                                    .withColumn('trip_hour', insertCandidateTripDFCow.trip_hour + 5) \\\n                                    .withColumn('trip_id', lit(TO_BE_INSERTED_TRIP_ID)) \\\n                                    .drop(\"_hoodie_commit_time\") \\\n                                    .drop(\"_hoodie_commit_seqno\") \\\n                                    .drop(\"_hoodie_record_key\") \\\n                                    .drop(\"_hoodie_trip_date\") \\\n                                    .drop(\"_hoodie_file_name\")\n\ninsertTripDFCow.show(truncate=False)\n"}, {"cell_type": "code", "execution_count": 7, "id": "74e0dc66-5e49-4190-9285-37b3f6e11a18", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n|trip_id      |taxi_type|vendor_id|pickup_datetime    |dropoff_datetime   |pickup_location_id|dropoff_location_id|trip_date |\n+-------------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n|1760936629794|green    |2        |2019-03-10 09:26:21|2019-03-10 09:50:20|177               |89                 |2019-03-10|\n+-------------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n\n"}], "source": "# Original record with just a few fields\nspark.sql(f\"SELECT trip_id,taxi_type,vendor_id,pickup_datetime,dropoff_datetime,pickup_location_id,dropoff_location_id,trip_date \" \\\n          f\" FROM {DATABASE_NAME}.{COW_TABLE_NAME} \"\\\n          f\" WHERE {INSERT_CANDIDATE_PREDICATES}\") \\\n        .show(truncate=False)"}, {"cell_type": "code", "execution_count": 8, "id": "6d01133d-7e66-450a-a683-ab64c09da6d3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 15:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n|trip_id      |taxi_type|vendor_id|pickup_datetime    |dropoff_datetime   |pickup_location_id|dropoff_location_id|trip_date |\n+-------------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n|1786706865009|green    |2        |2019-03-10 14:26:21|2019-03-10 14:50:20|177               |89                 |2019-03-10|\n+-------------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# The record we want to insert - note its pickup_datetime and dropoff_datetime are different\ninsertTripDFCow.select(\"trip_id\",\"taxi_type\",\"vendor_id\",\"pickup_datetime\",\"dropoff_datetime\",\"pickup_location_id\",\"dropoff_location_id\",\"trip_date\") \\\n               .show(truncate=False)\n"}, {"cell_type": "markdown", "id": "bd2bd2f0-f937-4925-920b-a246a6956432", "metadata": {}, "source": "### 1.4. Update record generation"}, {"cell_type": "code", "execution_count": 9, "id": "bd63d505-40cc-41a6-af20-2b72cf2f3efc", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+--------------------+------------------+----------------------+--------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+-----------+-----------+------------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|taxi_type|trip_year|trip_month|trip_day|trip_hour|trip_minute|vendor_id|    pickup_datetime|   dropoff_datetime|store_and_forward|rate_code|pickup_location_id|dropoff_location_id|passenger_count|trip_distance| fare_amount|  surcharge|    mta_tax|  tip_amount|tolls_amount|improvement_surcharge|total_amount|payment_type_code|congestion_surcharge|trip_type|ehail_fee|partition_date|distance_between_service|time_between_service|trip_id| trip_date|\n+-------------------+--------------------+------------------+----------------------+--------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+-----------+-----------+------------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------+----------+\n|  20230729053557449|20230729053557449...|                83|  trip_date=2019-01-15|1a4b14d7-2419-415...|   yellow|     2019|         1|      15|       10|         18|        2|2019-01-15 10:18:19|2019-01-15 10:19:23|                N|      2.0|               229|                229|              4|  0.040000000|52.000000000|0.300000000|0.500000000|11.710000000| 5.760000000|                 null|70.270000000|                1|                null|     null|     null|    2019-01-15|                    null|                null|     83|2019-01-15|\n+-------------------+--------------------+------------------+----------------------+--------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+-----------+-----------+------------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------+----------+\n\n"}], "source": "# Original record\nupdateCandidateTripDFCow=spark.sql(f\"SELECT * FROM {DATABASE_NAME}.{COW_TABLE_NAME} WHERE {UPDATE_CANDIDATE_PREDICATES}\")\nupdateCandidateTripDFCow.show()\n"}, {"cell_type": "code", "execution_count": 10, "id": "0f75df4c-51ce-46a7-ada1-219f45636da8", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+----------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+-----------+-----------+------------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------+----------+\n|_hoodie_partition_path|taxi_type|trip_year|trip_month|trip_day|trip_hour|trip_minute|vendor_id|pickup_datetime    |dropoff_datetime   |store_and_forward|rate_code|pickup_location_id|dropoff_location_id|passenger_count|trip_distance|fare_amount |surcharge  |mta_tax    |tip_amount  |tolls_amount|improvement_surcharge|total_amount|payment_type_code|congestion_surcharge|trip_type|ehail_fee|partition_date|distance_between_service|time_between_service|trip_id|trip_date |\n+----------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+-----------+-----------+------------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------+----------+\n|trip_date=2019-01-15  |yellow   |2019     |1         |15      |15       |18         |2        |2019-01-15 11:18:19|2019-01-15 11:19:23|N                |2.0      |229               |229                |4              |0.040000000  |52.000000000|0.300000000|0.500000000|11.710000000|5.760000000 |null                 |70.270000000|1                |null                |null     |null     |2019-01-15    |null                    |null                |83     |2019-01-15|\n+----------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+-----------+-----------+------------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Generate dataframe that updates the above record\nupdateTripDFCow = updateCandidateTripDFCow.withColumn('pickup_datetime', updateCandidateTripDFCow.pickup_datetime + F.expr('INTERVAL 1 HOURS')) \\\n                                    .withColumn('dropoff_datetime', updateCandidateTripDFCow.dropoff_datetime + F.expr('INTERVAL 1 HOURS')) \\\n                                    .withColumn('trip_hour', updateCandidateTripDFCow.trip_hour + 5) \\\n                                    .drop(\"_hoodie_commit_time\") \\\n                                    .drop(\"_hoodie_commit_seqno\") \\\n                                    .drop(\"_hoodie_record_key\") \\\n                                    .drop(\"_hoodie_trip_date\") \\\n                                    .drop(\"_hoodie_file_name\")\n\n# The full record we will update\nupdateTripDFCow.show(truncate=False)\n"}, {"cell_type": "code", "execution_count": 11, "id": "657f4d5a-51f8-4fc9-ab31-5df04f6d03f3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+-----------------------+-------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n|_hoodie_commit_time|_hoodie_commit_seqno   |trip_id|taxi_type|vendor_id|pickup_datetime    |dropoff_datetime   |pickup_location_id|dropoff_location_id|trip_date |\n+-------------------+-----------------------+-------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n|20230729053557449  |20230729053557449_478_0|83     |yellow   |2        |2019-01-15 10:18:19|2019-01-15 10:19:23|229               |229                |2019-01-15|\n+-------------------+-----------------------+-------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n\n"}], "source": "# Original record prior to update - just a few columns for readbility\nspark.sql(f\"SELECT _hoodie_commit_time,_hoodie_commit_seqno,trip_id,taxi_type,vendor_id,pickup_datetime,dropoff_datetime,pickup_location_id,dropoff_location_id,trip_date \" \\\n          f\" FROM {DATABASE_NAME}.{COW_TABLE_NAME} \"\\\n          f\" WHERE {UPDATE_CANDIDATE_PREDICATES}\") \\\n        .show(truncate=False)\n"}, {"cell_type": "code", "execution_count": 12, "id": "716dccc7-ee35-417b-94a7-839c7a25d803", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n|trip_id|taxi_type|vendor_id|pickup_datetime    |dropoff_datetime   |pickup_location_id|dropoff_location_id|trip_date |\n+-------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n|83     |yellow   |2        |2019-01-15 11:18:19|2019-01-15 11:19:23|229               |229                |2019-01-15|\n+-------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n\n"}], "source": "# Updated details of the above record - note its pickup_datetime and dropoff_datetime are different\nupdateTripDFCow.select(\"trip_id\",\"taxi_type\",\"vendor_id\",\"pickup_datetime\",\"dropoff_datetime\",\"pickup_location_id\",\"dropoff_location_id\",\"trip_date\") \\\n               .show(truncate=False)\n"}, {"cell_type": "markdown", "id": "3a70a400-80de-4554-a602-fcd7e73a0d58", "metadata": {}, "source": "### 1.5. Prepare to upsert"}, {"cell_type": "code", "execution_count": 13, "id": "4ddb13b1-8ff7-444a-a17c-c738c3cc18ad", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 25:===========================================>              (3 + 1) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+-----------+-----------+------------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------------+----------+\n|_hoodie_partition_path|taxi_type|trip_year|trip_month|trip_day|trip_hour|trip_minute|vendor_id|pickup_datetime    |dropoff_datetime   |store_and_forward|rate_code|pickup_location_id|dropoff_location_id|passenger_count|trip_distance|fare_amount |surcharge  |mta_tax    |tip_amount  |tolls_amount|improvement_surcharge|total_amount|payment_type_code|congestion_surcharge|trip_type|ehail_fee|partition_date|distance_between_service|time_between_service|trip_id      |trip_date |\n+----------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+-----------+-----------+------------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------------+----------+\n|trip_date=2019-03-10  |green    |2019     |3         |10      |14       |26         |2        |2019-03-10 14:26:21|2019-03-10 14:50:20|N                |5.0      |177               |89                 |1              |4.450000000  |15.360000000|0E-9       |0.500000000|0E-9        |0E-9        |null                 |15.860000000|1.0              |null                |2.0      |null     |2019-03-10    |null                    |null                |1786706865009|2019-03-10|\n|trip_date=2019-01-15  |yellow   |2019     |1         |15      |15       |18         |2        |2019-01-15 11:18:19|2019-01-15 11:19:23|N                |2.0      |229               |229                |4              |0.040000000  |52.000000000|0.300000000|0.500000000|11.710000000|5.760000000 |null                 |70.270000000|1                |null                |null     |null     |2019-01-15    |null                    |null                |83           |2019-01-15|\n+----------------------+---------+---------+----------+--------+---------+-----------+---------+-------------------+-------------------+-----------------+---------+------------------+-------------------+---------------+-------------+------------+-----------+-----------+------------+------------+---------------------+------------+-----------------+--------------------+---------+---------+--------------+------------------------+--------------------+-------------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Lets union the dataframes\nupsertTripDFCow = insertTripDFCow.union(updateTripDFCow)\n# Quick visual \nupsertTripDFCow.show(truncate=False)\n"}, {"cell_type": "code", "execution_count": 14, "id": "5e62553d-5f8a-4ef6-a8e9-d86bff524630", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Trip count before insert: 245530\n"}], "source": "# Capture record count before insert\nTRIP_COUNT_BEFORE_INSERT=spark.sql(f\"select count(*)  from {DATABASE_NAME}.{COW_TABLE_NAME} WHERE {INSERT_TRIP_DATE_PREDICATE}\").collect()[0][0]\nprint(f\"Trip count before insert: {TRIP_COUNT_BEFORE_INSERT}\")\n"}, {"cell_type": "code", "execution_count": 15, "id": "099f6aa8-a209-483e-be9b-9be1035f5661", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "     373 B  2023-07-30T01:59:34Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-03-10/.hoodie_partition_metadata.parquet#1690682374925116  metageneration=1\n  4.26 MiB  2023-07-30T01:59:34Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-03-10/7d98900a-981b-45a0-a5bf-bcbacbef7337-0_868-19-9830_20230729053557449.parquet#1690682374925584  metageneration=1\n   4.3 MiB  2023-07-30T01:59:34Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-03-10/adec5232-a984-4bd2-8e20-948dcf45b651-0_867-19-9829_20230729053557449.parquet#1690682374945161  metageneration=1\nTOTAL: 3 objects, 8984286 bytes (8.57 MiB)\n"}], "source": "# Capture GCS parquet file listing prior to insert\n!gsutil ls -alh $HUDI_COW_BASE_GCS_URI/trip_date=$INSERT_TRIP_DATE\n"}, {"cell_type": "code", "execution_count": 16, "id": "a83b51af-080e-4712-a261-2c47ed908d6b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Trip count before update: 289823\n"}], "source": "# Capture record count before update\nTRIP_COUNT_BEFORE_UPDATE=spark.sql(f\"select count(*)  from {DATABASE_NAME}.{COW_TABLE_NAME} WHERE {UPDATE_TRIP_DATE_PREDICATE}\").collect()[0][0]\nprint(f\"Trip count before update: {TRIP_COUNT_BEFORE_UPDATE}\")"}, {"cell_type": "code", "execution_count": 17, "id": "938ddd47-a4df-40c2-ba95-5239e4edf0db", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "     373 B  2023-07-30T01:59:33Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-01-15/.hoodie_partition_metadata.parquet#1690682373367806  metageneration=1\n  1.69 MiB  2023-07-30T01:59:33Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-01-15/02882cd4-4d59-4b22-9a13-797c96667c99-0_479-19-9441_20230729053557449.parquet#1690682373389182  metageneration=1\n  4.23 MiB  2023-07-30T01:59:33Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-01-15/1a4b14d7-2419-4151-8474-de33d473db5f-0_478-19-9440_20230729053557449.parquet#1690682373423858  metageneration=1\n  4.22 MiB  2023-07-30T01:59:33Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-01-15/e8680840-8f56-452f-83b6-77acd4227926-0_477-19-9439_20230729053557449.parquet#1690682373392460  metageneration=1\nTOTAL: 4 objects, 10638261 bytes (10.15 MiB)\n"}], "source": "# Capture GCS parquet file listing prior to update\n!gsutil ls -alh $HUDI_COW_BASE_GCS_URI/trip_date=$UPDATE_TRIP_DATE\n"}, {"cell_type": "code", "execution_count": 18, "id": "e993a545-1e59-4a53-9c38-7f72a05ed5e1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+----------------------------------------------------------------------------+-------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n|_hoodie_commit_time|_hoodie_file_name                                                           |trip_id|taxi_type|vendor_id|pickup_datetime    |dropoff_datetime   |pickup_location_id|dropoff_location_id|trip_date |\n+-------------------+----------------------------------------------------------------------------+-------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n|20230729053557449  |1a4b14d7-2419-4151-8474-de33d473db5f-0_478-19-9440_20230729053557449.parquet|83     |yellow   |2        |2019-01-15 10:18:19|2019-01-15 10:19:23|229               |229                |2019-01-15|\n+-------------------+----------------------------------------------------------------------------+-------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n\n"}], "source": "# Capture original record prior to update\nspark.sql(f\"SELECT _hoodie_commit_time,_hoodie_file_name,trip_id,taxi_type,vendor_id,pickup_datetime,dropoff_datetime,pickup_location_id,dropoff_location_id,trip_date \" \\\n          f\" FROM {DATABASE_NAME}.{COW_TABLE_NAME} \"\\\n          f\" WHERE {UPDATE_CANDIDATE_PREDICATES}\") \\\n        .show(truncate=False)"}, {"cell_type": "code", "execution_count": 19, "id": "59437434-3558-41c8-ad51-866ac2324a1b", "metadata": {}, "outputs": [], "source": "# HUDI options for the upsert operation\nhudi_options = {\n            'hoodie.database.name': DATABASE_NAME,\n            'hoodie.table.name': COW_TABLE_NAME,\n            'hoodie.datasource.write.table.name': COW_TABLE_NAME,\n            'hoodie.datasource.write.table.type': 'COPY_ON_WRITE',\n            'hoodie.datasource.write.recordkey.field': 'trip_id',\n            'hoodie.datasource.write.partitionpath.field': 'trip_date',\n            'hoodie.datasource.write.precombine.field': 'pickup_datetime',\n            'hoodie.datasource.write.hive_style_partitioning': 'true',\n            'hoodie.partition.metafile.use.base.format': 'true', \n            'hoodie.datasource.write.drop.partition.columns': 'true',\n            'hoodie.datasource.write.operation': 'upsert'   \n        }\n"}, {"cell_type": "markdown", "id": "e74b7c8a-ef7f-4311-a62d-700bbfe6b0db", "metadata": {}, "source": "### 1.6. Execute the upsert"}, {"cell_type": "code", "execution_count": 20, "id": "2372524b-90fc-4e85-9136-de18d1d76b62", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/07/30 02:29:49 WARN GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=109; previousMaxLatencyMs=0; operationCount=1; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/.hoodie/20230730022946927.deltacommit.requested\n23/07/30 02:30:00 WARN GhfsStorageStatistics: Detected potential high latency for operation op_get_file_status. latencyMs=290; previousMaxLatencyMs=286; operationCount=46; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/.hoodie/.aux/.bootstrap/.partitions/00000000-0000-0000-0000-000000000000-0_1-0-1_00000000000001.hfile\n23/07/30 02:30:00 WARN GhfsStorageStatistics: Detected potential high latency for operation op_create. latencyMs=449; previousMaxLatencyMs=94; operationCount=3; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/.hoodie/20230730022946927.deltacommit.inflight\n23/07/30 02:30:01 WARN GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=114; previousMaxLatencyMs=109; operationCount=2; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/.hoodie/20230730022946927.deltacommit.inflight\n23/07/30 02:30:02 WARN GhfsStorageStatistics: Detected potential high latency for operation stream_write_close_operations. latencyMs=183; previousMaxLatencyMs=114; operationCount=3; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/.hoodie/.temp/20230730022946927/MARKERS.type\n23/07/30 02:30:13 WARN GhfsStorageStatistics: Detected potential high latency for operation op_delete. latencyMs=107; previousMaxLatencyMs=0; operationCount=1; context=gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/.hoodie/metadata/.hoodie/.temp/20230730022946927\n"}], "source": "# Append to dataset in GCS\nupsertTripDFCow.write.format(\"hudi\"). \\\n                options(**hudi_options). \\\n                mode(\"append\"). \\\n                save(HUDI_COW_BASE_GCS_URI)\n"}, {"cell_type": "code", "execution_count": 21, "id": "739fb283-9f62-4590-8d0b-6d9af918cd49", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "++\n||\n++\n++\n\n"}], "source": "# Refresh Hive Metsatore Metadata\nspark.sql(f\"REFRESH TABLE {DATABASE_NAME}.{COW_TABLE_NAME};\").show(truncate=False)\n"}, {"cell_type": "markdown", "id": "8744b5c8-cb24-4d3a-bf4f-ea0705b0336d", "metadata": {}, "source": "### 1.7. Validate the insert"}, {"cell_type": "code", "execution_count": 22, "id": "6c89bc02-9b69-4023-88f9-7d72a8a1d537", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Trip count before insert was: 245530\n"}], "source": "print(f\"Trip count before insert was: {TRIP_COUNT_BEFORE_INSERT}\")\n"}, {"cell_type": "code", "execution_count": 23, "id": "f42b7c8b-0a1e-4111-93d4-5747fc1813b5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Trip count after insert: 245531\n"}], "source": "# Record count after insert\nTRIP_COUNT_AFTER_INSERT=spark.sql(f\"select count(*) from {DATABASE_NAME}.{COW_TABLE_NAME} WHERE {INSERT_TRIP_DATE_PREDICATE}\").collect()[0][0]\nprint(f\"Trip count after insert: {TRIP_COUNT_AFTER_INSERT}\")\n"}, {"cell_type": "code", "execution_count": 24, "id": "f79df3fd-3eb0-4849-be8f-1f7f68215fc0", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "     373 B  2023-07-30T01:59:34Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-03-10/.hoodie_partition_metadata.parquet#1690682374925116  metageneration=1\n  4.26 MiB  2023-07-30T02:30:06Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-03-10/7d98900a-981b-45a0-a5bf-bcbacbef7337-0_1-48-4123_20230730022946927.parquet#1690684206281955  metageneration=1\n  4.26 MiB  2023-07-30T01:59:34Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-03-10/7d98900a-981b-45a0-a5bf-bcbacbef7337-0_868-19-9830_20230729053557449.parquet#1690682374925584  metageneration=1\n   4.3 MiB  2023-07-30T01:59:34Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-03-10/adec5232-a984-4bd2-8e20-948dcf45b651-0_867-19-9829_20230729053557449.parquet#1690682374945161  metageneration=1\nTOTAL: 4 objects, 13454278 bytes (12.83 MiB)\n"}], "source": "# GCS parquet file listing after insert - note the extra file\n!gsutil ls -alh $HUDI_COW_BASE_GCS_URI/trip_date=$INSERT_TRIP_DATE\n"}, {"cell_type": "code", "execution_count": 25, "id": "23b7f326-8bf1-4ea7-9855-0376346050de", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------------------------------------------------------------+-------------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n|_hoodie_file_name                                                         |trip_id      |taxi_type|vendor_id|pickup_datetime    |dropoff_datetime   |pickup_location_id|dropoff_location_id|trip_date |\n+--------------------------------------------------------------------------+-------------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n|7d98900a-981b-45a0-a5bf-bcbacbef7337-0_1-48-4123_20230730022946927.parquet|1786706865009|green    |2        |2019-03-10 14:26:21|2019-03-10 14:50:20|177               |89                 |2019-03-10|\n+--------------------------------------------------------------------------+-------------+---------+---------+-------------------+-------------------+------------------+-------------------+----------+\n\n"}], "source": "# Check for existence of the record and the filename in which it exists\nspark.sql(f\"SELECT _hoodie_file_name,trip_id,taxi_type,vendor_id,pickup_datetime,dropoff_datetime,pickup_location_id,dropoff_location_id,trip_date \" \\\n          f\" FROM {DATABASE_NAME}.{COW_TABLE_NAME} \"\\\n          f\" WHERE {INSERT_TRIP_DATE_PREDICATE} AND trip_id={TO_BE_INSERTED_TRIP_ID}\") \\\n        .show(truncate=False)\n"}, {"cell_type": "markdown", "id": "3fbd3470-3a36-4eb0-a3b2-603bb52e5698", "metadata": {}, "source": "### 1.8. Validate the update"}, {"cell_type": "code", "execution_count": 26, "id": "dfeedf63-ba18-47c8-aa1a-aa4e4efdc953", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Trip count before update was: 289823\n"}], "source": "print(f\"Trip count before update was: {TRIP_COUNT_BEFORE_UPDATE}\")"}, {"cell_type": "code", "execution_count": 31, "id": "4ca314ee-a631-4ad1-970e-ce3fd6199d86", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Trip count after update: 289822\n"}], "source": "# Record count after update\nTRIP_COUNT_AFTER_UPDATE=spark.sql(f\"select count(*) from {DATABASE_NAME}.{COW_TABLE_NAME} WHERE {UPDATE_TRIP_DATE_PREDICATE}\").collect()[0][0]\nprint(f\"Trip count after update: {TRIP_COUNT_AFTER_UPDATE}\")"}, {"cell_type": "code", "execution_count": 28, "id": "78184877-8c0d-4f2f-96cb-b6709a2c4c36", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "  4.72 KiB  2023-07-30T02:30:03Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-01-15/.1a4b14d7-2419-4151-8474-de33d473db5f-0_20230729053557449.log.1_0-48-4122#1690684203324738  metageneration=1\n     373 B  2023-07-30T01:59:33Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-01-15/.hoodie_partition_metadata.parquet#1690682373367806  metageneration=1\n  1.69 MiB  2023-07-30T01:59:33Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-01-15/02882cd4-4d59-4b22-9a13-797c96667c99-0_479-19-9441_20230729053557449.parquet#1690682373389182  metageneration=1\n  4.23 MiB  2023-07-30T01:59:33Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-01-15/1a4b14d7-2419-4151-8474-de33d473db5f-0_478-19-9440_20230729053557449.parquet#1690682373423858  metageneration=1\n  4.22 MiB  2023-07-30T01:59:33Z  gs://gaia_data_bucket-623600433888/nyc-taxi-trips-hudi-cow/trip_date=2019-01-15/e8680840-8f56-452f-83b6-77acd4227926-0_477-19-9439_20230729053557449.parquet#1690682373392460  metageneration=1\nTOTAL: 5 objects, 10643098 bytes (10.15 MiB)\n"}], "source": "# GCS parquet file listing after insert\n!gsutil ls -alh $HUDI_COW_BASE_GCS_URI/trip_date=$UPDATE_TRIP_DATE"}, {"cell_type": "code", "execution_count": null, "id": "6dd99bf7-1260-4e03-9c58-df7a024c1ac1", "metadata": {}, "outputs": [{"ename": "NameError", "evalue": "name 'DATABASE_NAME' is not defined", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_218586/723616465.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check for update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m spark.sql(f\"SELECT _hoodie_commit_time,_hoodie_file_name,trip_id,taxi_type,vendor_id,pickup_datetime,dropoff_datetime,pickup_location_id,dropoff_location_id,trip_date\" \\\n\u001b[0;32m----> 3\u001b[0;31m           \u001b[0;34mf\" FROM {DATABASE_NAME}.{COW_TABLE_NAME} \"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m           f\" WHERE {UPDATE_TRIP_DATE_PREDICATE} AND trip_id=83\") \\\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'DATABASE_NAME' is not defined"]}, {"name": "stderr", "output_type": "stream", "text": "ERROR:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n    raise Py4JNetworkError(\"Answer from Java side is empty\")\npy4j.protocol.Py4JNetworkError: Answer from Java side is empty\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending or receiving\n/usr/lib/spark/python/pyspark/context.py:561: RuntimeWarning: Unable to cleanly shutdown Spark JVM process. It is possible that the process has crashed, been killed or may also be in a zombie state.\n  warnings.warn(\n"}], "source": "# Check for update\nspark.sql(f\"SELECT _hoodie_commit_time,_hoodie_file_name,trip_id,taxi_type,vendor_id,pickup_datetime,dropoff_datetime,pickup_location_id,dropoff_location_id,trip_date\" \\\n          f\" FROM {DATABASE_NAME}.{COW_TABLE_NAME} \"\\\n          f\" WHERE {UPDATE_TRIP_DATE_PREDICATE} AND trip_id={UPDATE_CANDIDATE_TRIP_ID}\") \\\n        .show(truncate=False)"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}, "toc-showcode": true}, "nbformat": 4, "nbformat_minor": 5}